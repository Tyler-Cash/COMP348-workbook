{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Simple Statistics and NLTK\n",
    "\n",
    "The following exercises use a portion of the Gutenberg corpus that is stored in the corpus dataset of NLTK. [The Project Gutenberg](http://www.gutenberg.org/) is a large collection of electronic books that are out of copyright. These books are free to download for reading, or for our case, for doing a little of corpus analysis.\n",
    "\n",
    "To obtain the list of files of NLTK's Gutenberg corpus, type the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\panda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To obtain all words in the entire Gutenberg corpus of NLTK, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gutenbergwords = nltk.corpus.gutenberg.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you can find the total number of words, and the first 10 words (do not attempt to display all the words or your computer will freeze!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenbergwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenbergwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can also find the words of just a selection of documents, as shown below. For more details of what information you can extract from this corpus, read the \"Gutenberg corpus\" section of the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 1.1\n",
    "*Find the words with length of at least 7 characters in the complete Gutenberg corpus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Woodhouse',\n",
       " 'handsome',\n",
       " 'comfortable',\n",
       " 'disposition',\n",
       " 'blessings',\n",
       " 'existence',\n",
       " 'distress',\n",
       " 'youngest',\n",
       " 'daughters',\n",
       " 'affectionate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater_than_seven = []\n",
    "for word in gutenbergwords:\n",
    "    if len(word) > 7:\n",
    "        greater_than_seven.append(word)\n",
    "print(str(len(greater_than_seven)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 1.2\n",
    "*Find the 5 most frequent words that are longer than 7 characters and occur more than 7 times in the complete Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('children', 2223),\n",
       " ('therefore', 1146),\n",
       " ('together', 958),\n",
       " ('something', 881),\n",
       " ('Jerusalem', 821)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "\n",
    "gutenbergwords = nltk.corpus.gutenberg.words()\n",
    "# finds all words with more than 7 characters\n",
    "greater_than_seven = []\n",
    "for word in gutenbergwords:\n",
    "    if len(word) > 7:\n",
    "        greater_than_seven.append(word)\n",
    "# finds words that occur more than 7 times\n",
    "frequence_of_words = collections.Counter(greater_than_seven)\n",
    "# outputs results\n",
    "large_frequent_words = collections.Counter(frequence_of_words)\n",
    "large_frequent_words.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 1.3\n",
    "*Find the average number of words across the documents of the NLTK Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145645.17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = 0\n",
    "for book in nltk.corpus.gutenberg.fileids():\n",
    "    total_words += len(nltk.corpus.gutenberg.words(book))\n",
    "total_words /= len(nltk.corpus.gutenberg.fileids())\n",
    "round(total_words, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 1.4\n",
    "*Find the Gutenberg document that has the longest average word length.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book with the largest average isbible-kjv.txt with a word count of 56147.44\n"
     ]
    }
   ],
   "source": [
    "largest_word_count = 0\n",
    "largest_book_name = \"\"\n",
    "for book in nltk.corpus.gutenberg.fileids():\n",
    "    total_words = len(nltk.corpus.gutenberg.words(book)) / len(nltk.corpus.gutenberg.fileids())\n",
    "    if total_words > largest_word_count:\n",
    "        largest_word_count = total_words\n",
    "        largest_book_name = book\n",
    "total_words /= len(nltk.corpus.gutenberg.fileids())\n",
    "print(\"The book with the largest average is\" + largest_book_name + \" with a word count of \" \\\n",
    "      + str(round(largest_word_count, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 1.5\n",
    "*Find the 10 most frequent bigrams in the entire Gutenberg corpus.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 41294)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.words()\n",
    "bigrams = nltk.bigrams(gutenbergwords)\n",
    "bigrams_frequency = collections.Counter(bigrams)\n",
    "bigrams_frequency.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 1.6\n",
    "*Find the most frequent bigram that begins with \"Moby\" in Herman Melville's \"Moby Dick\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Moby', 'Dick'), 83)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "bigrams = nltk.bigrams(gutenbergwords)\n",
    "moby_bigrams = []\n",
    "for bigram in bigrams:\n",
    "    if bigram[0] == \"Moby\":\n",
    "        moby_bigrams.append(bigram)\n",
    "moby_bigrams_frequency = collections.Counter(moby_bigrams)\n",
    "moby_bigrams_frequency.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Text Preprocessing with NLTK\n",
    "The following exercises will ask several questions about tokens, stems, and parts of speech.\n",
    "\n",
    "### Exercise 2.1\n",
    "*What is the sentence with the largest number of tokens in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest sentence is:\n",
      "\"While he lived, it must be only an engagement;\n",
      "but she flattered herself, that if divested of the danger of\n",
      "drawing her away, it might become an increase of comfort to him.--\n",
      "How to do her best by Harriet, was of more difficult decision;--\n",
      "how to spare her from any unnecessary pain; how to make\n",
      "her any possible atonement; how to appear least her enemy?--\n",
      "On these subjects, her perplexity and distress were very great--\n",
      "and her mind had to pass again and again through every bitter\n",
      "reproach and sorrowful regret that had ever surrounded it.--\n",
      "She could only resolve at last, that she would still avoid a\n",
      "meeting with her, and communicate all that need be told by letter;\n",
      "that it would be inexpressibly desirable to have her removed just\n",
      "now for a time from Highbury, and--indulging in one scheme more--\n",
      "nearly resolve, that it might be practicable to get an invitation\n",
      "for her to Brunswick Square.--Isabella had been pleased with Harriet;\n",
      "and a few weeks spent in London must give her some amusement.--\n",
      "She did not think it in Harriet's nature to escape being benefited\n",
      "by novelty and variety, by the streets, the shops, and the children.--\n",
      "At any rate, it would be a proof of attention and kindness in herself,\n",
      "from whom every thing was due; a separation for the present; an averting\n",
      "of the evil day, when they must all be together again.\" \n",
      "This sentence has 275 tokens\n"
     ]
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "most_sentence_tokens = 0\n",
    "largest_sentence = \"\"\n",
    "\n",
    "for sentence in nltk.sent_tokenize(words):\n",
    "    tokens = len(nltk.word_tokenize(sentence))\n",
    "    if tokens > most_sentence_tokens:\n",
    "        most_sentence_tokens = tokens\n",
    "        largest_sentence = sentence\n",
    "\n",
    "print(\"Largest sentence is:\\n\\\"\" + largest_sentence +\"\\\" \\nThis sentence has \" + str(most_sentence_tokens) + \" tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "An alternative solution that uses the pre-stored sentences provided by NLTK. We observe that these pre-stored sentences have the same problems as the ones produced by the sentence segmenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 2.2\n",
    "*What is the most frequent part of speech in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', ','), 12016)]\n"
     ]
    }
   ],
   "source": [
    "full_text = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "all_parts_of_speech = []\n",
    "\n",
    "for sentence in nltk.sent_tokenize(full_text):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    all_parts_of_speech += nltk.pos_tag(words)\n",
    "\n",
    "frequent_parts_of_speech = collections.Counter(all_parts_of_speech)\n",
    "print(frequent_parts_of_speech.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 2.3\n",
    "*What is the number of distinct stems in Austen's \"Emma\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5450"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stems = [stemmer.stem(word) for sentence in nltk.sent_tokenize(words)\n",
    "        for word in nltk.word_tokenize(sentence)]\n",
    "stems = set(stems)\n",
    "len(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 2.4\n",
    "*What is the most ambiguous stem in Austen's \"Emma\"? (meaning, which stem in Austen's \"Emma\" maps to the largest number of distinct tokens?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('respect',\n",
       " {'Respect',\n",
       "  'respect',\n",
       "  'respectability',\n",
       "  'respectable',\n",
       "  'respectably',\n",
       "  'respected',\n",
       "  'respectful',\n",
       "  'respectfully',\n",
       "  'respecting',\n",
       "  'respective',\n",
       "  'respects'})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stems = dict()\n",
    "for sentence in nltk.sent_tokenize(words):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        stemmed = stemmer.stem(word)\n",
    "        if stemmed not in stems:\n",
    "            stems[stemmed] = set([word])\n",
    "        else:\n",
    "            stems[stemmed].add(word)\n",
    "most_ambiguous = None\n",
    "number_stems = 0\n",
    "for key in stems:\n",
    "    if len(stems[key]) > number_stems:\n",
    "        number_stems = len(stems[key])\n",
    "        most_ambiguous = (key, stems[key])\n",
    "\n",
    "most_ambiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Regular Expressions\n",
    "The aim of this section is to develop a regular expression that detects all the numerical expressions in the Brown corpus. The Brown corpus is described in the [NLTK book chapter 2](http://www.nltk.org/book_1ed/ch02.html), section 2.1.\n",
    "\n",
    "The Brown corpus is a corpus annotated with the parts of speech. In the following exercises, you will concentrate on the words tagged with labels that begin with the `CD` tag. This tag stands for \"cardinal numeral\" and indicates that the token is a number. For the full list of tags in the Brown corpus you can see the [Wikipedia entry](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used).\n",
    "\n",
    "The following Python code shows the first 5 words of the \"news\" category of the Brown corpus. You'll see that it is a list of pairs, where each pair is a word and a tag. The tag has two components separated with `-`. We will focus on the labels that begin with `'CD'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('brown')\n",
    "tagged = nltk.corpus.brown.tagged_words(categories='news')\n",
    "tagged[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code stores the list of unannotated tokens in the variable `tokens`. Note that we could have used `nltk.corpus.brown.words` but build this list but the code below makes sure that the list mirrors the list `tagged'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [w for (w,t) in tagged]\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 3.1\n",
    "*Write code that finds all the numbers (items tagged with `'CD'`) from the list `tagged` and stores them in a new variable `numbers`. How many numbers are there? How many different numbers are there?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2020 numbers\n",
      "There are 460 distinct numbers\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "tagged = nltk.corpus.brown.tagged_words(categories='news')\n",
    "tagged[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 3.2\n",
    "*Find all the items in `tokens` that match the following regular expression: `^[0-9]+$`. Write the result as a list of pairs `(word,annotation)` so that, if the word is a number, the annotation is `'CD'`, and if it is not a number, the annotation is `''`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def annotateNum(listtokens):\n",
    "    \"\"\"Annotate the list of tokens to identify the numbers.\n",
    "    Example of run:\n",
    "    >>> annotateNum(['the','number','5'])\n",
    "    [('the', ''), ('number', ''), ('5', 'CD')]\n",
    "    \"\"\"\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', ''), ('number', ''), ('5', 'CD')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotateNum(['the','number','5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code computes the recall and precision of the annotations.\n",
    "\n",
    "1. The *recall* is the ratio of numbers that are tagged correctly.\n",
    "2. The *precision* is the ratio of tagged tokens that are numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate(result,tagged):\n",
    "    assert len(result) == len(tagged) # This is a check that the length of the result and tagged are equal\n",
    "    correct = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'CD' and tagged[i][1][:2] == 'CD']\n",
    "    numbers_result = [result[i][0] for i in range(len(result)) if result[i][1][:2] == 'CD']\n",
    "    numbers_tagged = [tagged[i][0] for i in range(len(tagged)) if tagged[i][1][:2] == 'CD']\n",
    "    print(\"Recall:\",len(correct)/len(numbers_tagged))\n",
    "    print(\"Precision:\",len(correct)/len(numbers_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5304428044280443\n",
      "Precision: 0.9982638888888888\n"
     ]
    }
   ],
   "source": [
    "evaluate(annotateNum(tokens),tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code returns the mistakes that you produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def false_positives(result,tagged):\n",
    "    \"Return the non-numbers that were tagged as numbers\"\n",
    "    return [tagged[i] for i in range(len(result)) if result[i][1]== 'CD' and tagged[i][1][:2] != 'CD']\n",
    "def false_negatives(result,tagged):\n",
    "    \"Return the numbers that were not tagged as numbers\"\n",
    "    return [result[i] for i in range(len(result)) if result[i][1]!= 'CD' and tagged[i][1][:2] == 'CD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 'OD'), ('3', 'OD-TL')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives(annotateNum(tokens),tagged)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('two', ''),\n",
       " ('one', ''),\n",
       " ('two', ''),\n",
       " ('Four', ''),\n",
       " ('one', ''),\n",
       " ('one', ''),\n",
       " ('one', ''),\n",
       " ('two', ''),\n",
       " ('Five', ''),\n",
       " ('three', '')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives(annotateNum(tokens),tagged)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercise 3.3\n",
    "**This exercise is optional for this workshop but it will be very useful for the sort of work that you will do in the assignments.**\n",
    "\n",
    "*Concentrate on your worst figure, either recall and precision, and try to improve it:*\n",
    "\n",
    "1. *If recall is low, list all the numbers that your system failed to detect (the false negatives). With those numbers in mind, update your regular expression.*\n",
    "2. *If precision is low, list all the tokens that your system erroneously classified as numbers (the false positives). The update your regular expression to cover those words.*\n",
    "\n",
    "*Test your new regular expression on the full Brown corpus, compute recall and precision, and see what you can improve. Do this several times and try to get better results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Solution: There's no single best pattern. The main goal of this activity is to\n",
    "help the students develop their analytical skills and to find a good\n",
    "methodology to develop rules. They could do this exercise in groups\n",
    "of two, and they should *really* test their patterns by computing\n",
    "recall and precision. I haven't introduced the concepts of recall and\n",
    "precision in the lectures yet, but hopefully they can see the usefulness\n",
    "of these measures for the development of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}